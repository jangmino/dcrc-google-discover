{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import font_manager\n",
    "from matplotlib import rc\n",
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "f_path = '/Library/Fonts/Arial Unicode.ttf'\n",
    "font_manager.FontProperties(fname=f_path).get_name()\n",
    "\n",
    "rc('font', family='Arial Unicode MS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from networkx.algorithms import bipartite, community\n",
    "from networkx.algorithms.community import modularity, louvain_communities\n",
    "import matplotlib.colors as mcolors\n",
    "from collections import Counter\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 읽기\n",
    "file_path = '../local_data/annotated_dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 데이터프레임에서 필요한 열만 추출\n",
    "df['keywords'] = df['keywords'].apply(lambda x: x.strip(\"[]\").split(\", \"))\n",
    "df['keywords'] = df['keywords'].apply(lambda x: [k.strip(\"'\") for k in x])\n",
    "\n",
    "df['title'] = df['title'].str.replace(r'^\\d+\\.\\s*', '', regex=True).str.strip()\n",
    "\n",
    "# # 데이터 확인\n",
    "# print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = list(mcolors.TABLEAU_COLORS.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다이나믹스 분석\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(group):\n",
    "    '''\n",
    "    카테고리 - 키워드 Bipartite 그래프 생성\n",
    "    '''\n",
    "    B = nx.Graph()\n",
    "    categories = group['category'].unique()\n",
    "    keywords = sum(group['keywords'].tolist(), [])\n",
    "    B.add_nodes_from(categories, bipartite=0)\n",
    "    B.add_nodes_from(keywords, bipartite=1)\n",
    "    for _, row in group.iterrows():\n",
    "        category = row['category']\n",
    "        for keyword in row['keywords']:\n",
    "            B.add_edge(category, keyword)\n",
    "    return B\n",
    "\n",
    "def calculate_authorities(B):\n",
    "    '''\n",
    "    HITS 알고리즘으로 hub와 authority 계산\n",
    "    \n",
    "    출력\n",
    "    - authorities: authority 값\n",
    "    - sorted_authorities: authority 값으로 정렬된 튜플 리스트\n",
    "    '''\n",
    "    hubs, authorities = nx.hits(B)\n",
    "    sorted_authorities = sorted(authorities.items(), key=lambda x: x[1], reverse=True)\n",
    "    return authorities, sorted_authorities\n",
    "\n",
    "def create_projected_graph(category_nodes, B, n):\n",
    "    '''\n",
    "    Bipartite 그래프로부터: 카테고리 노드 간의 연결을 기반으로한 Projected 그래프 생성\n",
    "    \n",
    "    내용\n",
    "    - 카테고리 노드 간의 최단 경로 길이가 n 이하인 경우에만 연결\n",
    "    '''\n",
    "    projected_graph = nx.Graph()\n",
    "    projected_graph.add_nodes_from(category_nodes)\n",
    "    for category in category_nodes:\n",
    "        lengths = nx.single_source_shortest_path_length(B, category, cutoff=n)\n",
    "        for target, length in lengths.items():\n",
    "            if target in category_nodes and category != target:\n",
    "                if projected_graph.has_edge(category, target):\n",
    "                    projected_graph[category][target]['weight'] += 1\n",
    "                else:\n",
    "                    projected_graph.add_edge(category, target, weight=1)\n",
    "    return projected_graph\n",
    "\n",
    "def detect_communities(projected_graph):\n",
    "    '''\n",
    "    louvain 알고리즘을 사용하여 커뮤니티 탐지\n",
    "    '''\n",
    "    louvain_communities_ = louvain_communities(projected_graph)\n",
    "    return louvain_communities_\n",
    "\n",
    "def calculate_community_scores(B, louvain_communities, projected_graph, authorities, keywords):\n",
    "    '''\n",
    "    projected_graph: 카테고리간 연결 그래프\n",
    "    louvain_communities: 이 그래프에서 찾은 커뮤니티\n",
    "    커뮤니티 내 각 키워드들이 가리키는 키워드의 집합 조회 -> connected_keywords\n",
    "    \n",
    "    출력: (중요한 순으로)\n",
    "    - Nodes: 커뮤니티를 구성하는 카테고리 모음\n",
    "    - Community: 커뮤니티 번호\n",
    "    \n",
    "    '''\n",
    "    community_scores = []\n",
    "    for i, comm in enumerate(louvain_communities):\n",
    "        comm_nodes = list(comm)\n",
    "        subgraph = projected_graph.subgraph(comm_nodes)\n",
    "        size = len(comm_nodes)\n",
    "        density = nx.density(subgraph)\n",
    "        connected_keywords = set()\n",
    "        for category in comm_nodes:\n",
    "            connected_keywords.update(B.neighbors(category))\n",
    "        keyword_count = len(connected_keywords)\n",
    "        keyword_ratio = keyword_count / len(keywords)\n",
    "        community_scores.append({\n",
    "            'Community': i + 1,\n",
    "            'Size': size,\n",
    "            'Density': density,\n",
    "            'Keyword Count': keyword_count,\n",
    "            'Keyword Ratio': keyword_ratio,\n",
    "            'keywords': connected_keywords,\n",
    "            'Nodes': tuple(sorted(comm_nodes))\n",
    "        })\n",
    "    return community_scores\n",
    "\n",
    "def initial_pass(previous_communities, current_communities):\n",
    "    '''\n",
    "    c \\in C_{t-1}, c' \\in C_{t} 에 대해 \n",
    "    \n",
    "    c' == c: c'는 유지 (c: persisting)\n",
    "    '''\n",
    "    \n",
    "    for cur in current_communities.keys():\n",
    "        for prev in previous_communities.keys():\n",
    "            if set(cur) == set(prev):\n",
    "                current_communities[cur]['type'] = ('persisting', prev)\n",
    "                previous_communities[prev]['type'] = ('persisting', cur)\n",
    "                break\n",
    "\n",
    "def first_pass(previous_communities, current_communities):\n",
    "    '''\n",
    "    c \\in C_{t-1}, c' \\in C_{t} 에 대해 \n",
    "    \n",
    "    c' 에 대해, c \\subset c' 인 c 모두 조사\n",
    "    - c 가 두가지 이상: c'는 병합 커뮤 (c: to_merge, to_merge, c': merge)\n",
    "    - c 가 한가지: c'는 확장 커뮤 (c 는 to_expand, c': expand)\n",
    "    - c 가 없음: c'는 신규 (c': new)\n",
    "    '''\n",
    "    \n",
    "    initial_pass(previous_communities, current_communities)\n",
    "    \n",
    "    for cur in current_communities.keys():\n",
    "        matched_prev = []\n",
    "        # if 'type' in current_communities[cur] and current_communities[cur]['type'][0] == 'persisting':\n",
    "        #     continue\n",
    "        exactly_matched = False\n",
    "        for prev in previous_communities.keys():\n",
    "            if set(cur) == set(prev):\n",
    "                exactly_matched = True\n",
    "                break\n",
    "            if set(prev).issubset(set(cur)):\n",
    "                matched_prev.append(prev)\n",
    "        if exactly_matched:\n",
    "            continue\n",
    "        if len(matched_prev) == 0:\n",
    "            current_communities[cur]['type'] = ('new', None)\n",
    "        elif len(matched_prev) == 1:\n",
    "            current_communities[cur]['type'] = ('expand_from', matched_prev)\n",
    "            for m in matched_prev:\n",
    "                previous_communities[m]['type'] = ('expand_to', cur)\n",
    "        else:\n",
    "            current_communities[cur]['type'] = ('merge_from', matched_prev)\n",
    "            for m in matched_prev:\n",
    "                previous_communities[m]['type'] = ('merge_to', cur)\n",
    "            \n",
    "def second_pass(previous_communities, current_communities):\n",
    "    '''\n",
    "    c \\in C_{t-1}, c' \\in C_{t} 에 대해 \n",
    "    \n",
    "    c 에 대해, c' \\subset c 인 c' 모두 조사\n",
    "    - c' 가 두가지 이상: c'는 분리 (c: to_split, c': split, spllit)\n",
    "    - c' 가 한가지: c'는 축소 (c: to_shrink, c': shrink)\n",
    "    - c' 가 없음: c'는 소멸 (c: to_disapper)\n",
    "    '''\n",
    "    \n",
    "    for prev in previous_communities.keys():\n",
    "        matched_curr = []\n",
    "        # if 'type' in previous_communities[prev] and previous_communities[prev]['type'][0] == 'persisting':\n",
    "        #     continue\n",
    "        exactly_matched = False\n",
    "        for cur in current_communities.keys():\n",
    "            if set(cur) == set(prev):\n",
    "                exactly_matched = True\n",
    "                break\n",
    "            if set(cur).issubset(set(prev)):\n",
    "                matched_curr.append(cur)\n",
    "        if exactly_matched:\n",
    "            continue\n",
    "        \n",
    "        if len(matched_curr) == 0:\n",
    "            # first_pass 에서 확장으로 결론 냈는데, 이를 다시 disappear로 바꾸는 것 방지\n",
    "            if 'type' in previous_communities[prev] and previous_communities[prev]['type'][0] in ['expand_to', 'merge_to']:\n",
    "                continue\n",
    "            previous_communities[prev]['type'] = ('disappear', None)\n",
    "        elif len(matched_curr) == 1:\n",
    "            previous_communities[prev]['type'] = ('shrink_to', matched_curr)\n",
    "            for m in matched_curr:\n",
    "                current_communities[m]['type'] = ('shrink_from', prev)\n",
    "        else:\n",
    "            previous_communities[prev]['type'] = ('split_to', matched_curr)\n",
    "            for m in matched_curr:\n",
    "                current_communities[m]['type'] = ('split_from', prev)\n",
    "\n",
    "\n",
    "def analyze_community_dynamics(previous_communities, current_communities, authorities, trial_index):\n",
    "    '''\n",
    "    커뮤니티 다이나믹스 분석\n",
    "    \n",
    "    입력\n",
    "    - previous_communities: 이전 트라이얼의 커뮤니티\n",
    "    - current_communities: 현재 트라이얼의 커뮤니티\n",
    "    '''\n",
    "    \n",
    "    # initial_pass(previous_communities, current_communities)\n",
    "    first_pass(previous_communities, current_communities)\n",
    "    second_pass(previous_communities, current_communities)\n",
    "    \n",
    "    print(f\"Trial {trial_index}:\")\n",
    "    \n",
    "    # 분석 0: P: persisting\n",
    "    set_P = set()\n",
    "    for cur in current_communities.keys():\n",
    "        if current_communities[cur]['type'][0] == 'persisting':\n",
    "            print(f\"P {cur}\")\n",
    "            set_P.add(cur)\n",
    "    \n",
    "    # 분석 1: N: new\n",
    "    set_N = set()\n",
    "    for cur in current_communities.keys():\n",
    "        if current_communities[cur]['type'][0] == 'new':\n",
    "            set_N.add(cur)\n",
    "            print(f\"N {cur}\")\n",
    "            \n",
    "    # 분석 2: D: disappear\n",
    "    set_D = set()\n",
    "    for prev in previous_communities.keys():\n",
    "        if previous_communities[prev]['type'][0] == 'disappear':\n",
    "            set_D.add(prev)\n",
    "            print(f\"D {prev}\")\n",
    "            \n",
    "    # 분석 3: E: expand, M: merge, SH: shrink, SP: split\n",
    "    set_E, set_E_from = set(), set()\n",
    "    set_M, set_M_from = set(), set()\n",
    "    set_SH, set_SH_from = set(), set()\n",
    "    set_SP, set_SP_from = set(), set()\n",
    "    for cur in current_communities.keys():\n",
    "        if current_communities[cur]['type'][0] == 'expand_from':\n",
    "            prev = current_communities[cur]['type'][1]\n",
    "            set_E.add(cur)\n",
    "            set_E_from |= set(prev) # 튜플의 리스트이므로, set으로 변환\n",
    "            print(f\"E {cur} from {prev}\")\n",
    "        elif current_communities[cur]['type'][0] == 'merge_from':\n",
    "            prev = current_communities[cur]['type'][1]\n",
    "            set_M.add(cur)\n",
    "            set_M_from |= set(prev) # 튜플의 리스트이므로, set으로 변환\n",
    "            print(f\"M {cur} from {prev}\")\n",
    "        elif current_communities[cur]['type'][0] == 'shrink_from':\n",
    "            prev = current_communities[cur]['type'][1]\n",
    "            set_SH.add(cur)\n",
    "            set_SH_from.add(prev)\n",
    "            print(f\"SH {cur} from {prev}\")\n",
    "        elif current_communities[cur]['type'][0] == 'split_from':\n",
    "            prev = current_communities[cur]['type'][1]\n",
    "            set_SP.add(cur)\n",
    "            set_SP_from.add(prev)\n",
    "            print(f\"SP {cur} from {prev}\")\n",
    "            \n",
    "    print(\"***************\")\n",
    "    \n",
    "\n",
    "    return (\n",
    "        set_N, # 신규\n",
    "        set_D, # 삭제\n",
    "        set_P, # 유지\n",
    "        set_E, set_E_from, # 확장\n",
    "        set_M, set_M_from, # 병합\n",
    "        set_SH, set_SH_from, # 축소\n",
    "        set_SP, set_SP_from, # 분리\n",
    "    )\n",
    "\n",
    "def visualize_community_counts(community_counts, tester):\n",
    "    trial_indices, counts = zip(*community_counts)\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.plot(trial_indices, counts, marker='o')\n",
    "    plt.xlabel('Trial Index')\n",
    "    plt.ylabel('Number of Communities')\n",
    "    plt.title(f'Number of Communities Over Time: {tester}')\n",
    "    plt.show()\n",
    "\n",
    "def visualize_community_sizes(community_sizes, tester):\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    for trial_index, sizes in community_sizes:\n",
    "        plt.scatter([trial_index] * len(sizes), sizes)\n",
    "    plt.xlabel('Trial Index')\n",
    "    plt.ylabel('Community Size (Keyword Count)')\n",
    "    plt.title(f'Community Size (Keyword Count) Over Time: {tester}')\n",
    "    plt.show()\n",
    "\n",
    "def visualize_community_persistence(community_persistence, tester):\n",
    "    persisting_counts = [entry['persisting'] for entry in community_persistence]\n",
    "    new_counts = [entry['new'] for entry in community_persistence]\n",
    "    disappearing_counts = [entry['disappearing'] for entry in community_persistence]\n",
    "    shrinking_counts = [entry['shrinking'] for entry in community_persistence]\n",
    "    expanding_counts = [entry['expanding'] for entry in community_persistence]\n",
    "    persisting_trial_indices = [entry['trial_idx'] for entry in community_persistence]\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.plot(persisting_trial_indices, persisting_counts, marker='o', label='Persisting')\n",
    "    plt.plot(persisting_trial_indices, new_counts, marker='o', label='New')\n",
    "    plt.plot(persisting_trial_indices, disappearing_counts, marker='o', label='Disappearing')\n",
    "    plt.plot(persisting_trial_indices, shrinking_counts, marker='o', label='Shrinking')\n",
    "    plt.plot(persisting_trial_indices, expanding_counts, marker='o', label='Expanding')\n",
    "    plt.xlabel('Trial Index')\n",
    "    plt.ylabel('Number of Communities')\n",
    "    plt.title(f'Community Persistence Over Time: {tester}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_community_movement(community_movement, tester):\n",
    "    movement_df = pd.DataFrame(community_movement)\n",
    "    if not movement_df.empty:\n",
    "        movement_df['color'] = movement_df['community'].apply(lambda x: colors[x % len(colors)])\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        for community, group in movement_df.groupby('community'):\n",
    "            plt.plot(group['trial_idx'], group['prev_center'], marker='o', linestyle='-', label=f'Community {community} Prev', color=group['color'].iloc[0])\n",
    "            plt.plot(group['trial_idx'], group['curr_center'], marker='x', linestyle='--', label=f'Community {community} Curr', color=group['color'].iloc[0])\n",
    "        plt.xlabel('Trial Index')\n",
    "        plt.ylabel('Center Node')\n",
    "        plt.title(f'Community Center Movement Over Time: {tester}')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "def print_category_stats(counter, title, max_len=5):\n",
    "    sorted_categories = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(f\"\\n{title}:\")\n",
    "    for i, (categories, count) in enumerate(sorted_categories):\n",
    "        print(f\"{categories}: {count}\")\n",
    "        if max_len > 0 and i == max_len - 1:\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_loop(df, tester):\n",
    "    filtered_df = df[df['tester'] == tester]\n",
    "    grouped = filtered_df.groupby('trial_idx')\n",
    "    graphs = []\n",
    "    community_dynamics = []\n",
    "    n = 8\n",
    "    previous_communities = {}\n",
    "    community_counts = []\n",
    "    community_sizes = []\n",
    "    community_persistence = []\n",
    "    community_movement = []\n",
    "    persisting_communities_counter = Counter()\n",
    "    disappearing_communities_counter = Counter()\n",
    "    new_communities_counter = Counter()\n",
    "    shrinking_communities_counter = Counter()\n",
    "    shrinking_communities_from_counter = Counter()\n",
    "    expanding_communities_counter = Counter()\n",
    "    expanding_communities_from_counter = Counter()\n",
    "    merging_communities_counter = Counter()\n",
    "    merging_communities_from_counter = Counter()\n",
    "    split_communities_counter = Counter()\n",
    "    split_communities_from_counter = Counter()\n",
    "\n",
    "    for trial_index, group in grouped:\n",
    "        B = create_graph(group)\n",
    "        graphs.append((trial_index, B))\n",
    "        authorities, sorted_authorities = calculate_authorities(B)\n",
    "        category_nodes = set(group['category'].unique())\n",
    "        projected_graph = create_projected_graph(category_nodes, B, n)\n",
    "        louvain_communities_ = detect_communities(projected_graph)\n",
    "        community_scores = calculate_community_scores(B, louvain_communities_, projected_graph, authorities, sum(group['keywords'].tolist(), []))\n",
    "        for comm in community_scores:\n",
    "            comm['prev_comm'] = None\n",
    "            comm['next_comm'] = None\n",
    "        current_communities = {comm['Nodes']: comm for comm in community_scores}\n",
    "        if previous_communities:\n",
    "            (\n",
    "                new_communities,\n",
    "                disappearing_communities,\n",
    "                persisting_communities,\n",
    "                expanding_communities, expanding_communities_from,\n",
    "                merging_communities, merging_communities_from,\n",
    "                shrinking_communities, shrinking_communities_from,\n",
    "                split_communities, split_communities_from,\n",
    "                \n",
    "            ) = analyze_community_dynamics(\n",
    "                previous_communities, current_communities, authorities, trial_index\n",
    "            )\n",
    "\n",
    "            community_persistence.append({\n",
    "                \"trial_idx\": trial_index,\n",
    "                \"new\": len(new_communities),\n",
    "                \"disappearing\": len(disappearing_communities),\n",
    "                \"persisting\": len(persisting_communities),\n",
    "                \"shrinking\": len(shrinking_communities),\n",
    "                \"expanding\": len(expanding_communities),\n",
    "                \"merging\": len(merging_communities),\n",
    "                \"split\": len(split_communities),\n",
    "                \"history\": {\"new\": new_communities, \n",
    "                            \"disappearing\": disappearing_communities, \n",
    "                            \"persisting\": persisting_communities, \n",
    "                            \"shrinking\": shrinking_communities,\n",
    "                            \"shrinking_from\": shrinking_communities_from,\n",
    "                            \"expanding\": expanding_communities, \n",
    "                            \"expanding_from\": expanding_communities_from,\n",
    "                            \"merging\": merging_communities, \n",
    "                            \"merging_from\": merging_communities_from,\n",
    "                            \"split\": split_communities, \n",
    "                            \"split_from\": split_communities_from,\n",
    "                            # merge_split_info\": from_to_persistent_communities\n",
    "                            }\n",
    "            })\n",
    "            persisting_communities_counter.update(persisting_communities)\n",
    "            new_communities_counter.update(new_communities)\n",
    "            disappearing_communities_counter.update(disappearing_communities)\n",
    "            shrinking_communities_counter.update(shrinking_communities)\n",
    "            shrinking_communities_from_counter.update(shrinking_communities_from)\n",
    "            expanding_communities_counter.update(expanding_communities)\n",
    "            expanding_communities_from_counter.update(expanding_communities_from)\n",
    "            merging_communities_counter.update(merging_communities)\n",
    "            merging_communities_from_counter.update(merging_communities_from)\n",
    "            split_communities_counter.update(split_communities)\n",
    "            split_communities_from_counter.update(split_communities_from)\n",
    "            \n",
    "            # for comm_nodes in persisting_communities:\n",
    "            #     prev_comm = previous_communities[comm_nodes]\n",
    "            #     curr_comm = current_communities[comm_nodes]\n",
    "            #     prev_center = max(prev_comm['Nodes'], key=lambda node: authorities[node])\n",
    "            #     curr_center = max(curr_comm['Nodes'], key=lambda node: authorities[node])\n",
    "            #     community_movement.append({\n",
    "            #         'trial_idx': trial_index,\n",
    "            #         'community': curr_comm['Community'],\n",
    "            #         'prev_center': prev_center,\n",
    "            #         'curr_center': curr_center\n",
    "            #     })\n",
    "        previous_communities = current_communities\n",
    "        sorted_communities = sorted(community_scores, key=lambda x: (x['Keyword Count'], x['Density']), reverse=True)\n",
    "        for comm in sorted_communities:\n",
    "            # print(f\"Community {comm['Community']}: Size={comm['Size']}, Density={comm['Density']:.4f}, Keyword Count={comm['Keyword Count']}, Keyword Ratio={comm['Keyword Ratio']:.4f}\")\n",
    "            # print(f\"Nodes: {', '.join(comm['Nodes'])}\")\n",
    "            comm_authorities = [(node, authorities[node]) for node in comm['Nodes']]\n",
    "            comm_authorities_sorted = sorted(comm_authorities, key=lambda x: x[1], reverse=True)\n",
    "            # for rank, (node, score) in enumerate(comm_authorities_sorted, start=1):\n",
    "            #     print(f\"  {node}: Authority={score:.4f}, Rank={rank}\")\n",
    "        community_counts.append((trial_index, len(louvain_communities_)))\n",
    "        community_sizes.append((trial_index, [comm['Keyword Count'] for comm in sorted_communities]))\n",
    "\n",
    "    # visualize_community_counts(community_counts, tester)\n",
    "    # visualize_community_sizes(community_sizes, tester)\n",
    "    visualize_community_persistence(community_persistence, tester)\n",
    "    # visualize_community_movement(community_movement, tester)\n",
    "    print_category_stats(persisting_communities_counter, \"Persisting Communities\")\n",
    "    print_category_stats(disappearing_communities_counter, \"Disappearing Communities\")\n",
    "    print_category_stats(new_communities_counter, \"New Communities\")\n",
    "    print_category_stats(shrinking_communities_counter, \"Shrinking Communities\")\n",
    "    print_category_stats(shrinking_communities_from_counter, \"Shrinking Communities From\")\n",
    "    print_category_stats(expanding_communities_counter, \"Expanding Communities\")\n",
    "    print_category_stats(expanding_communities_from_counter, \"Expanding Communities From\")\n",
    "    print_category_stats(merging_communities_counter, \"Merged Communities\")\n",
    "    print_category_stats(merging_communities_from_counter, \"Merged Communities From\")\n",
    "    print_category_stats(split_communities_counter, \"Split Communities\")\n",
    "    print_category_stats(split_communities_from_counter, \"Split Communities From\")\n",
    "    \n",
    "    return community_persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_testers = df['tester'].unique()\n",
    "analysis_results = []\n",
    "for tester in unique_testers:\n",
    "    print(f\"========== START: Analyzing {tester} ==========\")\n",
    "    analysis_results.append({\"tester\":tester, \"result\":analyze_loop(df, tester)})\n",
    "    print(f\"========== END: Analyzing {tester} ==========\")\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_results[0]['result'][1]['history']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다이나믹스 분석 결과 정비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_to_dataframe(dic):\n",
    "    result = dic['result']\n",
    "    data = []\n",
    "    for entry in result:\n",
    "        trial_idx = entry['trial_idx']\n",
    "        for key, value in entry['history'].items():\n",
    "            for comm in value:\n",
    "                data.append({\n",
    "                    'tester': dic['tester'],\n",
    "                    'trial_idx': trial_idx,\n",
    "                    'type': key,\n",
    "                    'community': comm\n",
    "                })\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_persisting_length(df:pd.DataFrame):\n",
    "    persisting_df = df.query(\"type == 'persisting'\").sort_values(by=['community', 'trial_idx'])\n",
    "    # 연속 여부 계산\n",
    "    persisting_df['prev_trial_idx'] = persisting_df.groupby('community')['trial_idx'].shift(1)\n",
    "    persisting_df['streak'] = (persisting_df['trial_idx'] == persisting_df['prev_trial_idx'] + 1).astype(int)\n",
    "\n",
    "    # 연속된 횟수 누적\n",
    "    persisting_df['streak_count'] = persisting_df.groupby('community')['streak'].cumsum() + 1\n",
    "\n",
    "    # 필요없는 컬럼 제거\n",
    "    persisting_df.drop(columns=['prev_trial_idx', 'streak'], inplace=True)\n",
    "    \n",
    "    return pd.merge(df, persisting_df.streak_count, left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dic = {}\n",
    "for dic in analysis_results:\n",
    "    df_ = calc_persisting_length(result_to_dataframe(dic))\n",
    "    summary_dic[dic['tester']] = df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle로 dict 저장\n",
    "# with open('../local_data/dynamics_summary_dict.pkl', 'wb') as file:\n",
    "with open('../local_data/dynamics_summary_dict_20241102.pkl', 'wb') as file:\n",
    "    pickle.dump(summary_dic, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 엑셀과 다이나믹스 서머리 병합\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 접두어 제거 함수 정의\n",
    "def remove_prefix(title):\n",
    "    return re.sub(r'^\\d+\\.\\s*', '', title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_file_path = r'../local_data/(2024-04-29)구글 디스커버_데이터_수집.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sheets = pd.read_excel(raw_file_path, sheet_name=None)\n",
    "new_cols = ['url-title', 'source', 'issue-dt', 'title', 'url', 'precision', 'freshness',\n",
    "       'satisfaction', 'trial_idx', 'dt']\n",
    "for key in all_sheets.keys():\n",
    "    all_sheets[key].columns = new_cols\n",
    "    all_sheets[key]['title'] = all_sheets[key]['title'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## category_assigned_sheets 제작\n",
    "\n",
    "타이틀마다 category, keywords 결합\n",
    "\n",
    "**오류**: `유효상`의 경우 하나의 라인 수가 다름!!!\n",
    "\n",
    "조사해보니 (엑셀 vs gpt 취합)\n",
    "- Length mismatch: 유효상: 375 vs 374\n",
    "- Length mismatch: 최지원: 617 vs 579"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(all_sheets.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_assigned_sheets = {}\n",
    "for tester in all_sheets.keys():\n",
    "    aaa = all_sheets[tester][['source', 'issue-dt', 'title', 'precision', 'freshness',\n",
    "        'satisfaction', 'trial_idx', 'dt']]\n",
    "    # bbb = df[df.tester==tester].reset_index(drop=True)[['title','category', 'keywords']]\n",
    "    bbb = df[df.tester==tester][['title','category', 'keywords', 'trial_idx']]\n",
    "    \n",
    "    # if len(aaa) != len(bbb):\n",
    "    #     print(f\"Length mismatch: {tester}: {len(aaa)} vs {len(bbb)}\")\n",
    "    \n",
    "    category_assigned_sheets[tester] = pd.merge(aaa, bbb, left_on=['title', 'trial_idx'], right_on=['title', 'trial_idx'])\n",
    "    print(f\"Length matched: {tester}: {len(category_assigned_sheets[tester])} <- {len(aaa)} vs {len(bbb)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다이나믹스 분석 데이터 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../local_data/dynamics_summary_dict.pkl', 'rb') as file:\n",
    "with open('../local_data/dynamics_summary_dict_20241102.pkl', 'rb') as file:\n",
    "    dynamics_summary_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 현재 기준 취합\n",
    "\n",
    "new, persisting\n",
    "\n",
    "```text\n",
    "new             76\n",
    "persisting      64\n",
    "disappearing    61\n",
    "split           16\n",
    "expanding        7\n",
    "merging          7\n",
    "shrinking        5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_indexes = list(range(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_community_info(category:str, today_info:pd.DataFrame):\n",
    "    founds = today_info['community_str'].str.contains(re.escape(category))\n",
    "    result = today_info[founds]\n",
    "    if result.empty:\n",
    "        raise Exception(f\"Category {category} not found\")\n",
    "    elif result.shape[0] > 1:\n",
    "        raise Exception(f\"Category {category} has multiple entries -> {result}\")\n",
    "    \n",
    "    return {\n",
    "        'type': result.iloc[0]['type'],        \n",
    "        'community': result.iloc[0]['community'],\n",
    "        'streak_count': result.iloc[0]['streak_count']\n",
    "    }\n",
    "\n",
    "def analyze_via_today(dynamics_info:pd.DataFrame, sheet_df:pd.DataFrame, trial_indexes:list):\n",
    "    datas = []\n",
    "    for trial_idx in trial_indexes[1:]:\n",
    "        today_info = dynamics_info.query(f\"trial_idx == {trial_idx} and not type.str.endswith('_from') and type != 'disappearing'\")\n",
    "        today_df = sheet_df.query(f\"trial_idx == {trial_idx}\")\n",
    "       \n",
    "        data = []\n",
    "        dummy_return = {\n",
    "        'type': 'dummy',\n",
    "        'community': 'dummy',\n",
    "        'streak_count': 0\n",
    "        }\n",
    "        for row in today_df.itertuples():\n",
    "            try:\n",
    "                result = search_community_info(row.category, today_info)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                result = dummy_return\n",
    "            data.append(result)\n",
    "        data_df = pd.DataFrame.from_records(data)\n",
    "        data_df.index = today_df.index\n",
    "        datas.append(data_df)\n",
    "    return pd.concat(datas, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_based_on_today = {}\n",
    "for tester in summary_dic.keys():\n",
    "    # tester='김태환'\n",
    "    print(f\".... Working on {tester}...\")\n",
    "    dynamics_info = dynamics_summary_dict[tester]\n",
    "    dynamics_info['community_str'] = dynamics_info.community.str.join('@')\n",
    "    sheet_df = category_assigned_sheets[tester]\n",
    "    result = pd.merge(sheet_df,\n",
    "             analyze_via_today(dynamics_info, sheet_df, trial_indexes), \n",
    "             left_index=True, right_index=True, how='left'\n",
    "             )\n",
    "    result_based_on_today[tester] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어제 기준 취합\n",
    "\n",
    "disappearing\n",
    "\n",
    "어제 등장한 카테고리가 오늘 사라졌음을 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_from_yesterday(dynamics_info:pd.DataFrame, sheet_df:pd.DataFrame, trial_indexes:list):\n",
    "    datas = []\n",
    "    for trial_idx in trial_indexes[1:]:\n",
    "        today_info = dynamics_info.query(f\"trial_idx == {trial_idx} and type == 'disappearing'\")\n",
    "        yesterday_df = sheet_df.query(f\"trial_idx == {trial_idx-1}\")\n",
    "       \n",
    "        data = []\n",
    "        dummy_return = {\n",
    "        'type': 'dummy',\n",
    "        'community': 'dummy',\n",
    "        'streak_count': 0\n",
    "        }\n",
    "        data_index = []\n",
    "        for row in yesterday_df.itertuples():\n",
    "            try:\n",
    "                result = search_community_info(row.category, today_info)\n",
    "                data.append(result)\n",
    "                data_index.append(row.Index)\n",
    "            except Exception as e:\n",
    "                # disappearing 만 조사할 것이라 없다면 그냥 패스함\n",
    "                pass\n",
    "\n",
    "        data_df = pd.DataFrame.from_records(data)\n",
    "        data_df.index = data_index\n",
    "        datas.append(data_df)\n",
    "    return pd.concat(datas, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_based_on_yesterday = {}\n",
    "for tester in summary_dic.keys():\n",
    "    # tester='김태환'\n",
    "    print(f\".... Working on {tester}...\")\n",
    "    dynamics_info = dynamics_summary_dict[tester]\n",
    "    dynamics_info['community_str'] = dynamics_info.community.str.join('@')\n",
    "    sheet_df = category_assigned_sheets[tester]\n",
    "    result = pd.merge(sheet_df,\n",
    "             analyze_from_yesterday(dynamics_info, sheet_df, trial_indexes), \n",
    "             left_index=True, right_index=True, how='left'\n",
    "             )\n",
    "    result_based_on_yesterday[tester] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_based_on_yesterday['김미령']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../local_data/dynamics_attached_sheet.pkl', 'wb') as file:\n",
    "with open('../local_data/dynamics_attached_sheet_20241102.pkl', 'wb') as file:\n",
    "    pickle.dump(result_based_on_today, file)\n",
    "    pickle.dump(result_based_on_yesterday, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
